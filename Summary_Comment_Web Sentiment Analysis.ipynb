{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# You will need to install the required libraries first in a Colab cell:\n",
        "!pip install langchain langchain-openai langchain-community youtube-transcript-api\n",
        "\n",
        "# --- 1. Import necessary libraries and securely access the API Key ---\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Securely get the API key from Colab secrets\n",
        "# This is the standard and secure way to handle API keys in Colab.\n",
        "# It replaces the need for python-dotenv and .env files.\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# It's also a good practice to set it as an environment variable\n",
        "# for libraries that might implicitly look for it.\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "\n",
        "# --- 2. Define the YouTube URL to be analyzed ---\n",
        "YOUTUBE_URL = \"https://www.youtube.com/watch?v=8KkKuTCFvzI\" # Example: Tim Urban's \"Inside the mind of a master procrastinator\"\n",
        "\n",
        "\n",
        "# --- 3. Create a ChatOpenAI model ---\n",
        "# The model will now use the API key we've set.\n",
        "model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "\n",
        "# --- 4. Define a function to load the video transcript ---\n",
        "def load_transcript(url: str) -> str:\n",
        "    \"\"\"Loads the transcript from a YouTube URL using YoutubeLoader.\"\"\"\n",
        "    try:\n",
        "        loader = YoutubeLoader.from_youtube_url(url, add_video_info=False, language=[\"en\", \"id\"])\n",
        "        documents = loader.load()\n",
        "        return \" \".join([doc.page_content for doc in documents])\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load transcript: {e}\")\n",
        "        return \"Transcript not available for this video.\"\n",
        "\n",
        "\n",
        "# --- 5. Define the prompt template for the summary ---\n",
        "summary_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a YouTube video critic. Your goal is to provide a clear and concise summary of the video based on its transcript.\"),\n",
        "        (\"human\", \"Please provide a brief summary of the video with the following transcript:\\n\\n---\\n\\n{transcript}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# --- 6. Create the chain using LangChain Expression Language (LCEL) ---\n",
        "chain = (\n",
        "    RunnableLambda(load_transcript)\n",
        "    | summary_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# --- 7. Run the chain ---\n",
        "print(f\"Generating summary for: {YOUTUBE_URL}\\n\")\n",
        "result = chain.invoke(YOUTUBE_URL)\n",
        "\n",
        "print(\"--- Summary ---\")\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-1.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.84.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.23-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-1.1.0-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.7/485.7 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, youtube-transcript-api, typing-inspect, pydantic-settings, langsmith, dataclasses-json, langchain-core, langchain-openai, langchain-community\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.44\n",
            "    Uninstalling langsmith-0.3.44:\n",
            "      Successfully uninstalled langsmith-0.3.44\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.63\n",
            "    Uninstalling langchain-core-0.3.63:\n",
            "      Successfully uninstalled langchain-core-0.3.63\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.25 langchain-core-0.3.65 langchain-openai-0.3.23 langsmith-0.3.45 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0 youtube-transcript-api-1.1.0\n",
            "Generating summary for: https://www.youtube.com/watch?v=8KkKuTCFvzI\n",
            "\n",
            "--- Summary ---\n",
            "The YouTube video is a profound lecture on the fundamentals of happiness and well-being, guided by the results of the longest study on adult life, the Harvard Study of Adult Development. The talk challenges the common perception of wealth and fame as primary life goals, proposing instead that true happiness and health are fostered predominantly by good relationships. Citing the 75-year study as evidence, the speaker emphasizes the importance of social connections, the quality of close relationships, and the security that strong relationships offer to mental health. The video encourages viewers to invest in building and maintaining strong relationships, arguing that these are crucial to health, happiness, and longevity. It ends with a profound Mark Twain quote underscoring the shortness of life and the importance of fostering love rather than grudges.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtAaCNgM2ZRq",
        "outputId": "d901fcfa-f0a0-452a-bf85-67de4127833a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Install the official Google API client library ---\n",
        "!pip install google-api-python-client -q\n",
        "\n",
        "# --- 2. Import necessary libraries ---\n",
        "import os\n",
        "from googleapiclient.discovery import build\n",
        "from google.colab import userdata\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# --- 3. Set up API Keys and Models ---\n",
        "# Make sure your OPENAI_API_KEY and YOUTUBE_API_KEY are set in Colab Secrets.\n",
        "\n",
        "# Set up OpenAI key and model (if not already done in a previous cell)\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# Get the YouTube API Key from secrets\n",
        "YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
        "\n",
        "# Define the video URL\n",
        "YOUTUBE_URL = \"https://www.youtube.com/watch?v=8KkKuTCFvzI\"\n",
        "\n",
        "\n",
        "# --- 4. Function to Fetch Comments using the YouTube Data API ---\n",
        "def get_video_comments(api_key, video_url, max_results=30):\n",
        "    \"\"\"\n",
        "    Fetches comments from a YouTube video using the YouTube Data API v3.\n",
        "    \"\"\"\n",
        "    comments = []\n",
        "    try:\n",
        "        # We need the video ID, which is the part of the URL after \"v=\"\n",
        "        video_id = video_url.split(\"=\")[1].split(\"&\")[0]\n",
        "\n",
        "        # Build the YouTube service object\n",
        "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "        # Make the API request to fetch the comment threads\n",
        "        request = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat='plainText'\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        # Extract the comment text from the response\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        return comments, None  # Return comments and no error\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return an empty list and the error message\n",
        "        return [], str(e)\n",
        "\n",
        "# --- 5. Fetch and Analyze the comments ---\n",
        "print(\"Fetching comments using the YouTube Data API...\")\n",
        "comments_list, error = get_video_comments(YOUTUBE_API_KEY, YOUTUBE_URL)\n",
        "\n",
        "if error:\n",
        "    print(f\"\\nAn error occurred while fetching comments: {error}\")\n",
        "    print(\"Please check the following:\\n1. Your YouTube API Key is correct and active.\\n2. The 'YouTube Data API v3' is enabled in your Google Cloud project.\\n3. The video has comments enabled.\")\n",
        "else:\n",
        "    print(f\"Successfully fetched {len(comments_list)} comments.\")\n",
        "\n",
        "    # Prepare comments for the LLM if we have any\n",
        "    if comments_list:\n",
        "        all_comments_text = \"\\n\\n---\\n\\n\".join(comments_list)\n",
        "\n",
        "        # Define the prompt template for analysis\n",
        "        analysis_prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\"system\", \"You are an expert at analyzing social media comments. Your task is to read a list of YouTube comments and provide a summary of the overall sentiment and key discussion points.\"),\n",
        "                (\"human\", \"\"\"Please analyze the following YouTube comments:\n",
        "\n",
        "<comments>\n",
        "{comment_list}\n",
        "</comments>\n",
        "\n",
        "Based on these comments, please provide:\n",
        "1.  **Overall Sentiment:** What is the general feeling (e.g., overwhelmingly positive, mixed, mostly negative)?\n",
        "2.  **Key Themes:** What are the 2-3 main topics or ideas that people are repeatedly talking about?\n",
        "3.  **Constructive Feedback or Questions:** Are there any common suggestions or questions being asked in the comments?\n",
        "4.  **A Memorable Quote:** Pick one comment that you think best represents the overall reaction.\"\"\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Create and run the analysis chain\n",
        "        analysis_chain = analysis_prompt | model | StrOutputParser()\n",
        "        print(\"\\nAnalyzing comments with the LLM...\")\n",
        "        analysis_result = analysis_chain.invoke({\"comment_list\": all_comments_text})\n",
        "\n",
        "        print(\"\\n--- Comment Analysis ---\")\n",
        "        print(analysis_result)\n",
        "    else:\n",
        "        print(\"\\nNo comments were fetched to analyze.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMTasxUa6uZ_",
        "outputId": "9deb64ce-2776-4b86-99d5-ecba963ccc9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching comments using the YouTube Data API...\n",
            "Successfully fetched 30 comments.\n",
            "\n",
            "Analyzing comments with the LLM...\n",
            "\n",
            "--- Comment Analysis ---\n",
            "1. **Overall Sentiment:** The general sentiment appears to be mixed. There are quite a few positive comments about the content, appreciating the insights provided, and expressing personal growth. However, there is also significant criticism concerning the gender exclusivity of the study discussed.\n",
            "   \n",
            "2. **Key Themes:** The main discussion points include: (a) the importance of relationships and social bonds in leading a good life and achieving longevity, (b) commentary and reactions to the study discussed in the talk, particularly its focus only on men and not including women, and (c) apparent testimonials related to someone named 'mansaarnault' and the beneficial effects of their rituals on people's lives.\n",
            "\n",
            "3. **Constructive Feedback or Questions:** The most common feedback and question is about the study's focus only on men and exclusion of women. Viewers are questioning the validity and relevance of the study given this issue.\n",
            "\n",
            "4. **A Memorable Quote:** \"A good life is built with good relationships. Thanks for your speaking, as sometimes I ignore some relationships around me, thought that I can improve it in other's time... but not. It's lifelong to take care the true relationship, maybe with parents, friends, colleague,â€¦ take care it before we cannot fix it.\" This comment best represents the overall emphasis on the importance of relationships as a key to happiness and a good, long life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Web & YouTube Creator Sentiment Analysis Agent\n",
        "\n",
        "# Step 1: Install necessary packages\n",
        "!pip install -q langchain langgraph langchain_openai python-dotenv tavily-python google-api-python-client\n",
        "\n",
        "# Step 2: Import libraries and set up API keys from Colab secrets\n",
        "import os\n",
        "import re\n",
        "from google.colab import userdata\n",
        "from typing import TypedDict, Annotated, Optional, List\n",
        "import json\n",
        "from uuid import uuid4\n",
        "\n",
        "from langgraph.graph import StateGraph, END, add_messages\n",
        "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Set API keys from Colab secrets\n",
        "try:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "    YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERROR: Secrets not found. Please ensure you have 'OPENAI_API_KEY', 'TAVILY_API_KEY', and 'YOUTUBE_API_KEY' set in your Colab secrets (left panel -> ğŸ”‘).\")\n",
        "    # Exit the script if keys are not found\n",
        "    exit()\n",
        "\n",
        "# Step 3: Define the tools and the state for the graph\n",
        "\n",
        "# Initialize the Tavily Search tool\n",
        "search_tool = TavilySearchResults(max_results=10, name=\"tavily_search_results_json\")\n",
        "\n",
        "def extract_video_id(url: str) -> Optional[str]:\n",
        "    \"\"\"Extracts the YouTube video ID from a URL using regex.\"\"\"\n",
        "    patterns = [\n",
        "        r\"(?:v=|\\/v\\/|youtu\\.be\\/|embed\\/|\\/v\\/|\\/e\\/|watch\\?v=|\\?v=)([^#\\&\\?]*).*\",\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    return None\n",
        "\n",
        "@tool\n",
        "async def get_youtube_channel_info(video_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches the channel title (creator's name) for a given YouTube video URL.\n",
        "    This should be the first tool used to identify the creator.\n",
        "    \"\"\"\n",
        "    from googleapiclient.discovery import build\n",
        "\n",
        "    video_id = extract_video_id(video_url)\n",
        "    if not video_id:\n",
        "        return {\"error\": \"Invalid YouTube URL provided. Could not extract video ID.\"}\n",
        "\n",
        "    try:\n",
        "        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
        "\n",
        "        video_response = youtube.videos().list(\n",
        "            part='snippet',\n",
        "            id=video_id\n",
        "        ).execute()\n",
        "\n",
        "        if not video_response.get('items'):\n",
        "            return {\"error\": \"Video not found.\"}\n",
        "\n",
        "        channel_id = video_response['items'][0]['snippet']['channelId']\n",
        "\n",
        "        channel_response = youtube.channels().list(\n",
        "            part='snippet',\n",
        "            id=channel_id\n",
        "        ).execute()\n",
        "\n",
        "        if not channel_response.get('items'):\n",
        "            return {\"error\": \"Channel not found.\"}\n",
        "\n",
        "        channel_title = channel_response['items'][0]['snippet']['title']\n",
        "        return {\"channel_name\": channel_title}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"An error occurred while fetching video/channel details: {str(e)}\"}\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    \"\"\"The state of our graph, which is a list of messages.\"\"\"\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Step 4: Define the graph components\n",
        "\n",
        "# Set up the AI model and bind the tools to it\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "tools = [search_tool, get_youtube_channel_info]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# Define the nodes of the graph\n",
        "async def agent_node(state: State):\n",
        "    \"\"\"Invokes the LLM to get a response or decide on a tool call.\"\"\"\n",
        "    response = await llm_with_tools.ainvoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "async def tool_node(state: State):\n",
        "    \"\"\"Executes the tool calls requested by the agent.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls:\n",
        "        return {\"messages\": []}\n",
        "\n",
        "    tool_messages = []\n",
        "    for tool_call in last_message.tool_calls:\n",
        "        tool_name = tool_call[\"name\"]\n",
        "        tool_to_call = next((t for t in tools if t.name == tool_name), None)\n",
        "\n",
        "        if not tool_to_call:\n",
        "            error_message = f\"Error: Tool '{tool_name}' not found.\"\n",
        "            tool_messages.append(ToolMessage(content=error_message, tool_call_id=tool_call[\"id\"]))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            tool_output = await tool_to_call.ainvoke(tool_call[\"args\"])\n",
        "            tool_messages.append(ToolMessage(\n",
        "                content=json.dumps(tool_output),\n",
        "                tool_call_id=tool_call[\"id\"]\n",
        "            ))\n",
        "        except Exception as e:\n",
        "            error_message = f\"Error executing tool '{tool_name}': {e}\"\n",
        "            tool_messages.append(ToolMessage(content=error_message, tool_call_id=tool_call[\"id\"]))\n",
        "\n",
        "    return {\"messages\": tool_messages}\n",
        "\n",
        "# Define the router logic\n",
        "def tool_router(state: State):\n",
        "    \"\"\"Checks the last message for tool calls and decides the next step.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0:\n",
        "        return \"tool_node\"\n",
        "    else:\n",
        "        return END\n",
        "\n",
        "# Step 5: Construct the graph\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"agent\", agent_node)\n",
        "graph_builder.add_node(\"tool_node\", tool_node)\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "graph_builder.add_conditional_edges(\"agent\", tool_router, {\"tool_node\": \"tool_node\", END: END})\n",
        "graph_builder.add_edge(\"tool_node\", \"agent\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph = graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "# Step 6: Define an async function to run the agent and stream the output\n",
        "async def run_analysis(youtube_url: str, thread_id: str):\n",
        "    \"\"\"Runs the full analysis process for a given YouTube URL.\"\"\"\n",
        "    print(f\"\\nAnalyzing sentiment for the creator of YouTube URL: '{youtube_url}'\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # The initial prompt guides the agent through the multi-step process.\n",
        "    initial_prompt = f\"\"\"\n",
        "    Please perform a sentiment analysis for the creator of the YouTube video at the following URL: {youtube_url}.\n",
        "\n",
        "    Your task involves three steps:\n",
        "    1.  First, use the `get_youtube_channel_info` tool to find the name of the content creator or channel.\n",
        "    2.  Next, take the channel name you just found and use it as the query for the `tavily_search_results_json` tool. This will gather general web sentiment and information about the creator.\n",
        "    3.  Finally, analyze all the search results to determine the overall public sentiment towards the creator/channel.\n",
        "\n",
        "    Your final answer should be a comprehensive report that includes:\n",
        "    - The name of the channel/creator you analyzed.\n",
        "    - The overall sentiment (e.g., Overwhelmingly Positive, Mixed, Mostly Negative, etc.).\n",
        "    - Key themes, praise, or criticisms found across the web search results.\n",
        "    - Reference at least 2-3 of the source URLs that informed your analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "\n",
        "    final_response = None\n",
        "    async for event in graph.astream_events(\n",
        "        {\"messages\": [HumanMessage(content=initial_prompt)]},\n",
        "        config=config,\n",
        "        version=\"v2\"\n",
        "    ):\n",
        "        kind = event[\"event\"]\n",
        "        if kind == \"on_chain_end\":\n",
        "            if event[\"name\"] == \"agent\" and event[\"data\"].get('output'):\n",
        "              final_response = event[\"data\"][\"output\"][\"messages\"][-1].content\n",
        "        elif kind == \"on_tool_start\":\n",
        "            print(f\"Tool Started: {event['name']} with args {event['data']['input']}\")\n",
        "        elif kind == \"on_tool_end\":\n",
        "            print(f\"Tool Ended: {event['name']}\")\n",
        "\n",
        "    print(\"\\n--- SENTIMENT ANALYSIS COMPLETE ---\")\n",
        "    print(final_response)\n",
        "\n",
        "# Step 7: Main execution block to run the analysis\n",
        "async def main():\n",
        "    # Prompt user for input\n",
        "    query = input(\"Please enter the YouTube video URL to analyze the creator's sentiment: \")\n",
        "    if not query:\n",
        "        print(\"No URL entered. Exiting.\")\n",
        "        return\n",
        "\n",
        "    thread_id = str(uuid4())\n",
        "    await run_analysis(query, thread_id)\n",
        "\n",
        "# Run the main function.\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUVSslHjRY4G",
        "outputId": "5d73f382-7b10-489f-b2e7-a157aa5d5fbb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-2020120895>:34: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  search_tool = TavilySearchResults(max_results=10, name=\"tavily_search_results_json\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the YouTube video URL to analyze the creator's sentiment: https://www.youtube.com/watch?v=egGAWaRwJgo\n",
            "\n",
            "Analyzing sentiment for the creator of YouTube URL: 'https://www.youtube.com/watch?v=egGAWaRwJgo'\n",
            "------------------------------\n",
            "Tool Started: get_youtube_channel_info with args {'video_url': 'https://www.youtube.com/watch?v=egGAWaRwJgo'}\n",
            "Tool Ended: get_youtube_channel_info\n",
            "Tool Started: tavily_search_results_json with args {'query': 'MONKEY MAJIK OFFICIAL CHANNEL'}\n",
            "Tool Ended: tavily_search_results_json\n",
            "\n",
            "--- SENTIMENT ANALYSIS COMPLETE ---\n",
            "### Sentiment Analysis Report for \"MONKEY MAJIK OFFICIAL CHANNEL\"\n",
            "\n",
            "**Channel/Creator Analyzed:** MONKEY MAJIK OFFICIAL CHANNEL\n",
            "\n",
            "**Overall Sentiment:** Mixed\n",
            "\n",
            "**Key Themes, Praise, and Criticisms:**\n",
            "\n",
            "1. **Popularity and Music Quality:**\n",
            "   - MONKEY MAJIK is known for producing popular music videos and has a dedicated fanbase. Several official music videos like \"O.G. Summer\" and \"fly\" are highlighted in their playlists, showcasing their musical diversity and appeal.\n",
            "   - The channel appears to maintain a solid engagement with its audience, as seen by the number of views and interactions across various videos.\n",
            "\n",
            "2. **Fans' Reception:**\n",
            "   - The general sentiment from comments and feedback on the videos is positive, with fans expressing appreciation for MONKEY MAJIK's unique sound and creative output. This is indicative of a loyal fan base who enjoys the content and music style.\n",
            "\n",
            "3. **Social Media Influence:**\n",
            "   - Apart from YouTube, MONKEY MAJIK also has a noticeable presence on other platforms like Instagram, suggesting a wider influence and reach.\n",
            "\n",
            "4. **Areas of Criticism:**\n",
            "   - While many comments are positive, there are few criticisms or negative reviews publicly available in the search results. This could either mean that the band does not have significant controversy or criticism or that such issues are less pronounced.\n",
            "\n",
            "5. **Channel Growth:**\n",
            "   - There are mentions of rapid subscriber growth, indicating that the channel occasionally gains notable traction and popularity spikes.\n",
            "\n",
            "**References:**\n",
            "- [MONKEY MAJIK on YouTube](https://www.youtube.com/playlist?list=PL17DitnKYFCgKRq-Y6becIXzNtVg_TQHd)\n",
            "- [MONKEY MAJIK Instagram Profile](https://www.instagram.com/monkeyxmagic/?hl=en)\n",
            "- [MONKEY MAJIK Official Channel on YouTube](https://www.youtube.com/channel/UCs9MYqANhxyLZJSkLGNx8FQ)\n",
            "\n",
            "Overall, MONKEY MAJIK is well-regarded for their contributions to music, with a stable reputation bolstered by support from a committed fan base. However, some aspects of broader public sentiment might be more nuanced or less publicly available based solely on this search.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}