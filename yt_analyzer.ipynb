{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OzasaHiro/NSFK/blob/main/yt_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_link_to_analyze = 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'"
      ],
      "metadata": {
        "id": "vjP1J85nyPbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_and_audio(video_filepath, temp_dir, frames_per_second=0.5):\n",
        "    \"\"\"\n",
        "    Extracts frames and audio from a video file, with adaptive sampling\n",
        "    based on scene changes and a base interval.\n",
        "\n",
        "    Args:\n",
        "        video_filepath (str): Path to the input video file.\n",
        "        temp_dir (str): Temporary directory to save extracted files.\n",
        "        frames_per_second (float): Base rate of frames to extract per second of video.\n",
        "                                  Scene changes will also be included.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of base64 encoded image strings, path to extracted audio file)\n",
        "               Returns ([], None) if an error occurs.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_filepath):\n",
        "        print(f\"Error: Video file not found at '{video_filepath}'\")\n",
        "        return [], None\n",
        "\n",
        "    cap = cv2.VideoCapture(video_filepath)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file '{video_filepath}'\")\n",
        "        return [], None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    video_duration_seconds = total_frames / fps\n",
        "\n",
        "    print(f\"Video Info: Total frames={total_frames}, FPS={fps:.2f}, Duration={video_duration_seconds:.2f}s\")\n",
        "\n",
        "    # --- Scene Detection ---\n",
        "    print(\"\\nDetecting scene changes...\")\n",
        "    scene_list = []\n",
        "    try:\n",
        "        # Use the updated detect_scenes function directly from scenedetect\n",
        "        # It takes the video path and a list of detectors.\n",
        "        # frame_skip=0 ensures every frame is processed for scene detection accuracy.\n",
        "        scene_list = detect_scenes(video_filepath, [ContentDetector(threshold=27.0)], frame_skip=0)\n",
        "        print(f\"Detected {len(scene_list)} scenes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during scene detection: {e}. Proceeding without scene-based sampling.\")\n",
        "        scene_list = [] # Fallback if scene detection fails\n",
        "\n",
        "\n",
        "    scene_change_frame_numbers = set()\n",
        "    for scene in scene_list:\n",
        "        # Add the start and end frames of each detected scene\n",
        "        scene_change_frame_numbers.add(scene[0].frame_number)\n",
        "        scene_change_frame_numbers.add(scene[1].frame_number)\n",
        "        # Also add a few frames around the scene change for more context\n",
        "        for offset in [-int(fps/2), -1, 1, int(fps/2)]: # e.g., 0.5 sec before/after, and immediate frames\n",
        "             if 0 <= (scene[0].frame_number + offset) < total_frames:\n",
        "                 scene_change_frame_numbers.add(scene[0].frame_number + offset)\n",
        "             if 0 <= (scene[1].frame_number + offset) < total_frames:\n",
        "                 scene_change_frame_numbers.add(scene[1].frame_number + offset)\n",
        "\n",
        "\n",
        "    # --- Frame Sampling and Conversion ---\n",
        "    cap = cv2.VideoCapture(video_filepath)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not re-open video file '{video_filepath}' for frame extraction.\")\n",
        "        return [], None\n",
        "\n",
        "    sampled_frames_data = {} # Use a dict to store {frame_number: base64_image} to avoid duplicates and maintain order\n",
        "\n",
        "    # Calculate interval for base sampling\n",
        "    frame_interval = int(fps / frames_per_second) if frames_per_second > 0 else 1\n",
        "    if frame_interval == 0:\n",
        "        frame_interval = 1 # Ensure at least one frame is picked if frames_per_second is very low\n",
        "\n",
        "    current_frame_number = 0\n",
        "    success, image = cap.read()\n",
        "\n",
        "    print(f\"Extracting frames (base: every {frame_interval}th frame, plus scene changes)...\")\n",
        "    while success:\n",
        "        # Include frame if it's on the regular interval OR if it's a scene change frame\n",
        "        if (current_frame_number % frame_interval == 0) or (current_frame_number in scene_change_frame_numbers):\n",
        "            # Encode frame to JPEG\n",
        "            _, buffer = cv2.imencode('.jpg', image)\n",
        "            # Base64 encode and store, using the frame number as key to avoid duplicates\n",
        "            sampled_frames_data[current_frame_number] = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "        success, image = cap.read()\n",
        "        current_frame_number += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Sort frames by their number to maintain chronological order\n",
        "    sorted_frame_numbers = sorted(sampled_frames_data.keys())\n",
        "    final_sampled_frames_base64 = [sampled_frames_data[fn] for fn in sorted_frame_numbers]\n",
        "\n",
        "    print(f\"Total unique frames extracted: {len(final_sampled_frames_base64)}\")\n",
        "\n",
        "    # --- Audio Extraction ---\n",
        "    audio_output_filepath = os.path.join(temp_dir, \"extracted_audio.wav\")\n",
        "    print(f\"Extracting audio to: {audio_output_filepath}\")\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(video_filepath)\n",
        "        audio.export(audio_output_filepath, format=\"wav\")\n",
        "        print(\"Audio extraction complete.\")\n",
        "        return final_sampled_frames_base64, audio_output_filepath\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting audio: {e}\")\n",
        "        print(\"Ensure FFmpeg is installed and accessible in your system's PATH for audio extraction.\")\n",
        "        return final_sampled_frames_base64, None"
      ],
      "metadata": {
        "id": "1sxeY7AhDyap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_scene_changes_opencv(video_filepath, threshold=2000000):\n",
        "    \"\"\"\n",
        "    Detects scene changes in a video using simple frame differencing with OpenCV.\n",
        "    A scene change is detected if the sum of absolute differences between\n",
        "    consecutive grayscale frames exceeds a threshold.\n",
        "\n",
        "    Args:\n",
        "        video_filepath (str): Path to the input video file.\n",
        "        threshold (int): The threshold for detecting a scene change.\n",
        "                         Higher value means less sensitive detection.\n",
        "                         Adjust based on video characteristics.\n",
        "\n",
        "    Returns:\n",
        "        set: A set of frame numbers where scene changes are detected.\n",
        "    \"\"\"\n",
        "    print(\"Detecting scene changes using OpenCV frame differencing...\")\n",
        "    scene_change_frames = set()\n",
        "    cap = cv2.VideoCapture(video_filepath)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video for scene detection: {video_filepath}\")\n",
        "        return scene_change_frames\n",
        "\n",
        "    ret, prev_frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        return scene_change_frames\n",
        "\n",
        "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    current_frame_number = 0\n",
        "\n",
        "    while True:\n",
        "        ret, current_frame = cap.read()\n",
        "        if not ret:\n",
        "            break # End of video\n",
        "\n",
        "        current_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Calculate the absolute difference between the current and previous grayscale frames\n",
        "        frame_diff = cv2.absdiff(current_gray, prev_gray)\n",
        "\n",
        "        # Sum up all pixel differences. If it's a large value, a scene change likely occurred.\n",
        "        diff_sum = np.sum(frame_diff)\n",
        "\n",
        "        if diff_sum > threshold:\n",
        "            # Mark the current frame as a scene change\n",
        "            scene_change_frames.add(current_frame_number)\n",
        "            # Also add the frame just before the cut for context\n",
        "            if current_frame_number > 0:\n",
        "                scene_change_frames.add(current_frame_number - 1)\n",
        "\n",
        "        prev_gray = current_gray\n",
        "        current_frame_number += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Detected {len(scene_change_frames)} scene changes.\")\n",
        "    return scene_change_frames\n",
        "\n",
        "\n",
        "def extract_frames_and_audio(video_filepath, temp_dir, frames_per_second=0.5, scene_threshold=2000000):\n",
        "    \"\"\"\n",
        "    Extracts frames and audio from a video file, with adaptive sampling\n",
        "    based on scene changes and a base interval.\n",
        "\n",
        "    Args:\n",
        "        video_filepath (str): Path to the input video file.\n",
        "        temp_dir (str): Temporary directory to save extracted files.\n",
        "        frames_per_second (float): Base rate of frames to extract per second of video.\n",
        "                                  Scene changes will also be included.\n",
        "        scene_threshold (int): Threshold for OpenCV scene detection.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list of base64 encoded image strings, path to extracted audio file)\n",
        "               Returns ([], None) if an error occurs.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_filepath):\n",
        "        print(f\"Error: Video file not found at '{video_filepath}'\")\n",
        "        return [], None\n",
        "\n",
        "    cap = cv2.VideoCapture(video_filepath)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file '{video_filepath}'\")\n",
        "        return [], None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    video_duration_seconds = total_frames / fps\n",
        "\n",
        "    print(f\"Video Info: Total frames={total_frames}, FPS={fps:.2f}, Duration={video_duration_seconds:.2f}s\")\n",
        "\n",
        "    # --- Scene Detection ---\n",
        "    # Call the custom OpenCV scene detection function\n",
        "    scene_change_frame_numbers = detect_scene_changes_opencv(video_filepath, threshold=scene_threshold)\n",
        "\n",
        "    # --- Frame Sampling and Conversion ---\n",
        "    # Re-open video capture as scene detection might have closed it\n",
        "    cap = cv2.VideoCapture(video_filepath)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not re-open video file '{video_filepath}' for frame extraction.\")\n",
        "        return [], None\n",
        "\n",
        "    sampled_frames_data = {} # Use a dict to store {frame_number: base64_image} to avoid duplicates and maintain order\n",
        "\n",
        "    # Calculate interval for base sampling\n",
        "    frame_interval = int(fps / frames_per_second) if frames_per_second > 0 else 1\n",
        "    if frame_interval == 0:\n",
        "        frame_interval = 1 # Ensure at least one frame is picked if frames_per_second is very low\n",
        "\n",
        "    current_frame_number = 0\n",
        "    success, image = cap.read()\n",
        "\n",
        "    print(f\"Extracting frames (base: every {frame_interval}th frame, plus {len(scene_change_frame_numbers)} scene changes)...\")\n",
        "    while success:\n",
        "        # Include frame if it's on the regular interval OR if it's a scene change frame\n",
        "        if (current_frame_number % frame_interval == 0) or (current_frame_number in scene_change_frame_numbers):\n",
        "            # Encode frame to JPEG\n",
        "            _, buffer = cv2.imencode('.jpg', image)\n",
        "            # Base64 encode and store, using the frame number as key to avoid duplicates\n",
        "            sampled_frames_data[current_frame_number] = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "        success, image = cap.read()\n",
        "        current_frame_number += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Sort frames by their number to maintain chronological order\n",
        "    sorted_frame_numbers = sorted(sampled_frames_data.keys())\n",
        "    final_sampled_frames_base64 = [sampled_frames_data[fn] for fn in sorted_frame_numbers]\n",
        "\n",
        "    print(f\"Total unique frames extracted: {len(final_sampled_frames_base64)}\")\n",
        "\n",
        "    # --- Audio Extraction ---\n",
        "    audio_output_filepath = os.path.join(temp_dir, \"extracted_audio.wav\")\n",
        "    print(f\"Extracting audio to: {audio_output_filepath}\")\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(video_filepath)\n",
        "        audio.export(audio_output_filepath, format=\"wav\")\n",
        "        print(\"Audio extraction complete.\")\n",
        "        return final_sampled_frames_base64, audio_output_filepath\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting audio: {e}\")\n",
        "        print(\"Ensure FFmpeg is installed and accessible in your system's PATH for audio extraction.\")\n",
        "        return final_sampled_frames_base64, None\n"
      ],
      "metadata": {
        "id": "ZrbqMh7QF7p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install whisper\n",
        "!pip install scenedetect # Install scenedetect\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2 # For video frame extraction\n",
        "import tempfile\n",
        "import base64\n",
        "import json\n",
        "import asyncio\n",
        "from pydub import AudioSegment # For audio extraction/conversion\n",
        "import whisper # For Speech-to-Text\n",
        "import numpy as np # For numerical operations with OpenCV frames\n",
        "\n",
        "# For scene detection\n",
        "from scenedetect import VideoManager\n",
        "from scenedetect import SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "\n",
        "# For actual Gemini API calls - Changing from 'requests' to 'aiohttp' for async calls\n",
        "import aiohttp # Install with: pip install aiohttp\n",
        "\n",
        "# Import nest_asyncio to handle running event loops in interactive environments\n",
        "try:\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "except ImportError:\n",
        "    print(\"nest_asyncio not found. If you encounter 'RuntimeError: Event loop is already running',\"\n",
        "          \" consider installing it: pip install nest_asyncio\")\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# You need a valid API key for gemini-2.0-flash.\n",
        "# In the Canvas environment, __api_key will be automatically provided if left empty.\n",
        "API_KEY = YOUR_YOUTUBE_API_KEY\n",
        "GEMINI_API_URL = YOUR_GEMINI_API_KEY\n",
        "\n",
        "def transcribe_audio_with_whisper(audio_file_path):\n",
        "    \"\"\"\n",
        "    Transcribes an audio file using the OpenAI Whisper model.\n",
        "    To speed up: consider using a smaller model ('tiny', 'base') or\n",
        "    ensure your system can leverage GPU if using 'openai-whisper' (PyTorch version).\n",
        "    \"\"\"\n",
        "    if not audio_file_path or not os.path.exists(audio_file_path):\n",
        "        print(\"No audio file for transcription or file not found.\")\n",
        "        return \"\"\n",
        "\n",
        "    print(\"\\nLoading Whisper model (this might take a moment)...\")\n",
        "    try:\n",
        "        # Choose a smaller model like 'tiny' or 'base' for faster transcription\n",
        "        # e.g., model = whisper_cpp.Whisper.from_pretrained(\"tiny\")\n",
        "        model = whisper.load_model(\"base\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Whisper model: {e}\")\n",
        "        print(\"Please ensure whisper-cpp-python is correctly installed and can download model files.\")\n",
        "        return \"\"\n",
        "\n",
        "    print(\"Transcribing audio...\")\n",
        "    try:\n",
        "        result = model.transcribe(audio_file_path)\n",
        "        transcription_text = result[\"text\"]\n",
        "        print(\"Transcription complete.\")\n",
        "        return transcription_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during transcription: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "async def analyze_frame_with_gemini(session, base64_image, frame_number, video_title):\n",
        "    \"\"\"\n",
        "    Sends a single base64 encoded image to Gemini 2.0 Flash for safety analysis asynchronously.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "    }\n",
        "\n",
        "    # Prompt specific to identify unsafe content for children\n",
        "    prompt_text = (\n",
        "        f\"Analyze this image from the video titled '{video_title}'. \"\n",
        "        f\"Specifically describe anything that could be potentially offensive, inappropriate, or unsafe \"\n",
        "        f\"for young children (e.g., violence, weapons, nudity, explicit content, drug/alcohol use, \"\n",
        "        f\"dangerous situations, scary imagery, jump scares, disturbing visual effects). \"\n",
        "        f\"If the image appears completely safe and normal for kids, respond with 'SAFE_FOR_KIDS'.\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": [\n",
        "                    {\"text\": prompt_text},\n",
        "                    {\"inlineData\": {\"mimeType\": \"image/jpeg\", \"data\": base64_image}}\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        async with session.post(f\"{GEMINI_API_URL}?key={API_KEY}\", headers=headers, json=payload) as response:\n",
        "            response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "            result = await response.json()\n",
        "\n",
        "            if result.get('candidates') and result['candidates'][0].get('content') and result['candidates'][0]['content'].get('parts'):\n",
        "                description = result['candidates'][0]['content']['parts'][0]['text'].strip()\n",
        "                if description == \"SAFE_FOR_KIDS\":\n",
        "                    # print(f\"  Frame {frame_number}: Safe.\") # Keep quiet for safe frames to reduce console spam\n",
        "                    return None # No unsafe content found for this frame\n",
        "                else:\n",
        "                    print(f\"  Frame {frame_number}: Potential Issue: {description[:100]}...\") # Print first 100 chars\n",
        "                    return f\"Frame {frame_number}: {description}\"\n",
        "            else:\n",
        "                print(f\"  Frame {frame_number}: No valid response from LLM.\")\n",
        "                return f\"Frame {frame_number}: (Frame {frame_number} - No specific analysis from LLM, check response structure)\"\n",
        "    except aiohttp.ClientError as e:\n",
        "        print(f\"  Frame {frame_number}: API request failed: {e}\")\n",
        "        # Correct indentation for the return statement\n",
        "        return f\"Frame {frame_number}: (Frame {frame_number} - API request failed: {e})\"\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"  Frame {frame_number}: Failed to decode JSON from LLM response.\")\n",
        "        # Correct indentation for the return statement\n",
        "        return f\"Frame {frame_number}: (Frame {frame_number} - JSON decode error from LLM)\"\n",
        "    except Exception as e:\n",
        "        print(f\"  Frame {frame_number}: Unexpected error during LLM analysis: {e}\")\n",
        "        # Correct indentation for the return statement\n",
        "        return f\"Frame {frame_number}: (Frame {frame_number} - Unexpected LLM analysis error: {e})\"\n",
        "\n",
        "\n",
        "async def video_safety_agent(video_filepath, video_title):\n",
        "    \"\"\"\n",
        "    Analyzes a local video's visuals and audio for child safety,\n",
        "    summarizing potentially offensive or unsafe content for parents.\n",
        "\n",
        "    Args:\n",
        "        video_filepath (str): The full path to the locally downloaded video file.\n",
        "        video_title (str): The title of the video (for context in analysis).\n",
        "\n",
        "    Returns:\n",
        "        str: A comprehensive safety summary for parents.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Video Safety Analysis for: '{video_title}' ---\")\n",
        "    print(f\"Video File: {video_filepath}\")\n",
        "\n",
        "    # Use a temporary directory for all intermediate files\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        print(f\"Using temporary directory: {temp_dir}\")\n",
        "\n",
        "        # Step 1: Extract frames and audio\n",
        "        # Adjust frames_per_second for base sampling rate.\n",
        "        # Scene change detection will add more frames where visual content changes.\n",
        "        sampled_frames_base64, audio_filepath = extract_frames_and_audio(video_filepath, temp_dir, frames_per_second=0.5)\n",
        "\n",
        "        # Step 2: Transcribe audio\n",
        "        audio_transcript = transcribe_audio_with_whisper(audio_filepath)\n",
        "        if not audio_transcript.strip():\n",
        "            print(\"Warning: No audio transcript generated or it's empty.\")\n",
        "\n",
        "        # Step 3: Analyze sampled frames with Multimodal LLM concurrently\n",
        "        print(\"\\n--- Analyzing video frames for unsafe content concurrently ---\")\n",
        "        visual_safety_tasks = []\n",
        "\n",
        "        # Use aiohttp.ClientSession for efficient concurrent HTTP requests\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            for i, frame_data in enumerate(sampled_frames_base64):\n",
        "                # Create a task for each frame analysis\n",
        "                task = analyze_frame_with_gemini(session, frame_data, i + 1, video_title)\n",
        "                visual_safety_tasks.append(task)\n",
        "\n",
        "            # Run all analysis tasks concurrently and wait for them to complete\n",
        "            # This is the main speedup for LLM calls by parallelizing API requests.\n",
        "            visual_safety_observations = await asyncio.gather(*visual_safety_tasks)\n",
        "            # Filter out None values (where frames were deemed SAFE_FOR_KIDS)\n",
        "            visual_safety_observations = [obs for obs in visual_safety_observations if obs is not None]\n",
        "\n",
        "        # Step 4: Aggregate all information and summarize with LLM\n",
        "        print(\"\\n--- Aggregating information for final safety summary ---\")\n",
        "\n",
        "        combined_analysis_text = f\"Video Title: '{video_title}'\\n\\n\"\n",
        "        combined_analysis_text += \"--- Audio Transcript ---\\n\"\n",
        "        if audio_transcript:\n",
        "            combined_analysis_text += audio_transcript + \"\\n\\n\"\n",
        "        else:\n",
        "            combined_analysis_text += \"No audio transcript available.\\n\\n\"\n",
        "\n",
        "        combined_analysis_text += \"--- Visual Observations from Sampled Frames ---\\n\"\n",
        "        if visual_safety_observations:\n",
        "            for obs in visual_safety_observations:\n",
        "                combined_analysis_text += obs + \"\\n\"\n",
        "        else:\n",
        "            combined_analysis_text += \"No specific unsafe visual content detected in sampled frames.\\n\"\n",
        "\n",
        "        # Final prompt to the LLM for comprehensive safety assessment\n",
        "        final_summary_prompt = (\n",
        "            \"Given the following information about a video, provide a comprehensive summary for parents \"\n",
        "            \"indicating whether this video is safe for young children. \"\n",
        "            \"Specifically highlight any content (visual or audio) that could be offensive, inappropriate, \"\n",
        "            \"or dangerous, and explain why. If no such content is found, state that it appears generally safe. \"\n",
        "            \"Consider themes of violence, explicit language, nudity, drug/alcohol use, scary imagery, \"\n",
        "            \"or depictions of dangerous situations, jump scares, disturbing visual effects, or rapid flashing lights. \"\n",
        "            \"Prioritize clear, concise language for parents, using bullet points for identified issues.\\n\\n\"\n",
        "            \"Here is the collected data:\\n\"\n",
        "            f\"{combined_analysis_text}\"\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- Final LLM call for overall safety assessment ---\")\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "        }\n",
        "        payload = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"parts\": [\n",
        "                        {\"text\": final_summary_prompt}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Using aiohttp for the final summary call as well for consistency\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                async with session.post(f\"{GEMINI_API_URL}?key={API_KEY}\", headers=headers, json=payload) as response:\n",
        "                    response.raise_for_status()\n",
        "                    result = await response.json()\n",
        "\n",
        "                    if result.get('candidates') and result['candidates'][0].get('content') and result['candidates'][0]['content'].get('parts'):\n",
        "                        final_summary = result['candidates'][0]['content']['parts'][0]['text'].strip()\n",
        "                        return final_summary\n",
        "                    else:\n",
        "                        return \"Error: Could not get a valid final summary from the LLM.\"\n",
        "        except aiohttp.ClientError as e:\n",
        "            return f\"Error: Final LLM API request failed: {e}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: Unexpected error during final LLM summary: {e}\"\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- IMPORTANT: Replace this with the actual path to your downloaded video file ---\n",
        "    # Example: local_video_path = \"C:/Users/YourUser/Videos/my_downloaded_youtube_video.mp4\"\n",
        "    # Example: local_video_path = \"/Users/YourUser/Downloads/my_downloaded_youtube_video.webm\"\n",
        "    local_video_path = input(\"Enter the full path to your downloaded YouTube video file: \")\n",
        "\n",
        "    # You might get the title from a filename or the YouTube Data API if needed\n",
        "    # For simplicity, we'll extract it from the filename for this example\n",
        "    # video_filename = os.path.basename(local_video_path)\n",
        "    # video_title_from_file = os.path.splitext(video_filename)[0] # Remove extension for title\n",
        "\n",
        "    # If you want to use a specific title regardless of filename:\n",
        "    # video_title_for_agent = \"My Awesome Kids Video Test\"\n",
        "    video_title_for_agent = \"Camp Camp: New Episodes March 1st!\" # Using extracted filename as title for demo\n",
        "\n",
        "    async def run_agent():\n",
        "        summary = await video_safety_agent(local_video_path, video_title_for_agent)\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Final Video Safety Report for Parents:\")\n",
        "        print(\"=\"*50)\n",
        "        print(summary)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    # Run the asynchronous function\n",
        "    asyncio.run(run_agent())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "ZN7DTbBMA3o2",
        "outputId": "15c11ea3-9283-4643-9ac9-d500df9a8785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: whisper in /usr/local/lib/python3.11/dist-packages (1.1.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n",
            "Collecting scenedetect\n",
            "  Downloading scenedetect-0.6.6-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from scenedetect) (8.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scenedetect) (2.0.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from scenedetect) (4.3.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scenedetect) (4.67.1)\n",
            "Downloading scenedetect-0.6.6-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scenedetect\n",
            "Successfully installed scenedetect-0.6.6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'YOUR_YOUTUBE_API_KEY' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1332128653>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# You need a valid API key for gemini-2.0-flash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# In the Canvas environment, __api_key will be automatically provided if left empty.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mAPI_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOUR_YOUTUBE_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mGEMINI_API_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOUR_GEMINI_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'YOUR_YOUTUBE_API_KEY' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}